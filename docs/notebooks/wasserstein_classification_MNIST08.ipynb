{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: HKR classifier on MNIST dataset\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deel-ai/deel-torchlip/blob/master/docs/notebooks/wasserstein_classification_MNIST08.ipynb)\n",
    "\n",
    "This notebook demonstrates how to learn a binary classifier on the MNIST0-8 dataset (MNIST with only 0 and 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required library deel-torchlip (uncomment line below)\n",
    "# %pip install -qqq deel-torchlip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, which\n",
    "is compatible with the hinge term used in the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 11774 samples, classes proportions: 50.31 %\n",
      "Test set size: 1954 samples, classes proportions: 50.15 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "# First we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(dataset, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function converts the MNIST data to make it suitable for our binary\n",
    "    classification setup.\n",
    "    \"\"\"\n",
    "    x = dataset.data\n",
    "    y = dataset.targets\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x = x.float() / 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "\n",
    "    y_ = torch.zeros_like(y).float()\n",
    "    y_[y == class_a] = 1.0\n",
    "    y_[y == class_b] = -1.0\n",
    "    return torch.utils.data.TensorDataset(x, y_)\n",
    "\n",
    "\n",
    "train = datasets.MNIST(\"./data\", train=True, download=True)\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True)\n",
    "\n",
    "# Prepare the data\n",
    "train = prepare_data(train, selected_classes[0], selected_classes[1])\n",
    "test = prepare_data(test, selected_classes[0], selected_classes[1])\n",
    "\n",
    "# Display infos about dataset\n",
    "print(\n",
    "    f\"Train set size: {len(train)} samples, classes proportions: \"\n",
    "    f\"{100 * (train.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set size: {len(test)} samples, classes proportions: \"\n",
    "    f\"{100 * (test.tensors[1] == 1).numpy().mean():.2f} %\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Lipschitz model\n",
    "\n",
    "Here, the experiments are done with a model with only fully-connected layers. However,\n",
    "`torchlip` also provides state-of-the-art 1-Lipschitz convolutional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): ParametrizedSpectralLinear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _SpectralNorm()\n",
       "        (1): _BjorckNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): FullSort()\n",
       "  (3): ParametrizedSpectralLinear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _SpectralNorm()\n",
       "        (1): _BjorckNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): FullSort()\n",
       "  (5): ParametrizedSpectralLinear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _SpectralNorm()\n",
       "        (1): _BjorckNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): FullSort()\n",
       "  (7): ParametrizedFrobeniusLinear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): _FrobeniusNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from deel import torchlip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ninputs = 28 * 28\n",
    "wass = torchlip.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torchlip.SpectralLinear(ninputs, 128),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(128, 64),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.SpectralLinear(64, 32),\n",
    "    torchlip.FullSort(),\n",
    "    torchlip.FrobeniusLinear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "wass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learn classification on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "loss: -0.0655 - KR: 3.3978 - acc: 0.9913 - val_loss: -0.0769 - val_KR: 4.2157 - val_acc: 0.9933\n",
      "Epoch 2/10\n",
      "loss: -0.1013 - KR: 4.7773 - acc: 0.9945 - val_loss: -0.0989 - val_KR: 5.3608 - val_acc: 0.9928\n",
      "Epoch 3/10\n",
      "loss: -0.0946 - KR: 5.6133 - acc: 0.9951 - val_loss: -0.1112 - val_KR: 5.9211 - val_acc: 0.9949\n",
      "Epoch 4/10\n",
      "loss: -0.1145 - KR: 6.0779 - acc: 0.9963 - val_loss: -0.1180 - val_KR: 6.2546 - val_acc: 0.9939\n",
      "Epoch 5/10\n",
      "loss: -0.1133 - KR: 6.2920 - acc: 0.9962 - val_loss: -0.1206 - val_KR: 6.3919 - val_acc: 0.9944\n",
      "Epoch 6/10\n",
      "loss: -0.1371 - KR: 6.5019 - acc: 0.9965 - val_loss: -0.1255 - val_KR: 6.6471 - val_acc: 0.9939\n",
      "Epoch 7/10\n",
      "loss: -0.1226 - KR: 6.6214 - acc: 0.9969 - val_loss: -0.1261 - val_KR: 6.7408 - val_acc: 0.9939\n",
      "Epoch 8/10\n",
      "loss: -0.1395 - KR: 6.7325 - acc: 0.9967 - val_loss: -0.1280 - val_KR: 6.7204 - val_acc: 0.9944\n",
      "Epoch 9/10\n",
      "loss: -0.1271 - KR: 6.7927 - acc: 0.9971 - val_loss: -0.1255 - val_KR: 6.8759 - val_acc: 0.9898\n",
      "Epoch 10/10\n",
      "loss: -0.1316 - KR: 6.8134 - acc: 0.9970 - val_loss: -0.1286 - val_KR: 6.8696 - val_acc: 0.9928\n"
     ]
    }
   ],
   "source": [
    "from deel.torchlip import KRLoss, HKRLoss, HingeMarginLoss\n",
    "\n",
    "# training parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 1\n",
    "alpha = 0.98\n",
    "\n",
    "kr_loss = KRLoss()\n",
    "hkr_loss = HKRLoss(alpha=alpha, min_margin=min_margin)\n",
    "hinge_margin_loss =HingeMarginLoss(min_margin=min_margin)\n",
    "optimizer = torch.optim.Adam(lr=0.001, params=wass.parameters())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    m_kr, m_hm, m_acc = 0, 0, 0\n",
    "    wass.train()\n",
    "\n",
    "    for step, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = wass(data)\n",
    "        loss = hkr_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics on batch\n",
    "        m_kr += kr_loss(output, target)\n",
    "        m_hm += hinge_margin_loss(output, target)\n",
    "        m_acc += (torch.sign(output).flatten() == torch.sign(target)).sum() / len(\n",
    "            target\n",
    "        )\n",
    "\n",
    "    # Train metrics for the current epoch\n",
    "    metrics = [\n",
    "        f\"{k}: {v:.04f}\"\n",
    "        for k, v in {\n",
    "            \"loss\": loss,\n",
    "            \"KR\": m_kr / (step + 1),\n",
    "            \"acc\": m_acc / (step + 1),\n",
    "        }.items()\n",
    "    ]\n",
    "\n",
    "    # Compute test loss for the current epoch\n",
    "    wass.eval()\n",
    "    testo = []\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        testo.append(wass(data).detach().cpu())\n",
    "    testo = torch.cat(testo).flatten()\n",
    "\n",
    "    # Validation metrics for the current epoch\n",
    "    metrics += [\n",
    "        f\"val_{k}: {v:.04f}\"\n",
    "        for k, v in {\n",
    "            \"loss\": hkr_loss(\n",
    "                testo, test.tensors[1]\n",
    "            ),\n",
    "            \"KR\": kr_loss(testo.flatten(), test.tensors[1]),\n",
    "            \"acc\": (torch.sign(testo).flatten() == torch.sign(test.tensors[1]))\n",
    "            .float()\n",
    "            .mean(),\n",
    "        }.items()\n",
    "    ]\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(\" - \".join(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Lipschitz constant of our networks\n",
    "\n",
    "### 4.1. Empirical evaluation\n",
    "\n",
    "We can estimate the Lipschitz constant by evaluating \n",
    "\n",
    "$$\n",
    "    \\frac{\\Vert{}F(x_2) - F(x_1)\\Vert{}}{\\Vert{}x_2 - x_1\\Vert{}} \\quad\\text{or}\\quad \n",
    "    \\frac{\\Vert{}F(x + \\epsilon) - F(x)\\Vert{}}{\\Vert{}\\epsilon\\Vert{}}\n",
    "$$\n",
    "\n",
    "for various inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1439)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "wass.eval()\n",
    "\n",
    "p = []\n",
    "for _ in range(64):\n",
    "    eps = 1e-3\n",
    "    batch, _ = next(iter(train_loader))\n",
    "    dist = torch.distributions.Uniform(-eps, +eps).sample(batch.shape)\n",
    "    y1 = wass(batch.to(device)).detach().cpu()\n",
    "    y2 = wass((batch + dist).to(device)).detach().cpu()\n",
    "\n",
    "    p.append(\n",
    "        torch.max(\n",
    "            torch.norm(y2 - y1, dim=1)\n",
    "            / torch.norm(dist.reshape(dist.shape[0], -1), dim=1)\n",
    "        )\n",
    "    )\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8923, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for batch, _ in train_loader:\n",
    "    x = batch.numpy()\n",
    "    y = wass(batch.to(device)).detach().cpu().numpy()\n",
    "    xd = pdist(x.reshape(batch.shape[0], -1))\n",
    "    yd = pdist(y.reshape(batch.shape[0], -1))\n",
    "\n",
    "    p.append((yd / xd).max())\n",
    "print(torch.tensor(p).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using the $\\epsilon$-version, we greatly under-estimate the Lipschitz constant.\n",
    "Using the train dataset, we find a Lipschitz constant close to 0.9, which is better, but our network should be 1-Lipschitz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Singular-Value Decomposition\n",
    "\n",
    "Since our network is only made of linear layers and `FullSort` activation, we can compute *Singular-Value Decomposition* (SVD) of our weight matrix and check that, for each linear layer, all singular values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before export ===\n",
      "ParametrizedSpectralLinear(\n",
      "  in_features=784, out_features=128, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): _SpectralNorm()\n",
      "      (1): _BjorckNorm()\n",
      "    )\n",
      "  )\n",
      "), min=0.9999998211860657, max=1.0\n",
      "ParametrizedSpectralLinear(\n",
      "  in_features=128, out_features=64, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): _SpectralNorm()\n",
      "      (1): _BjorckNorm()\n",
      "    )\n",
      "  )\n",
      "), min=0.9999998211860657, max=1.0000001192092896\n",
      "ParametrizedSpectralLinear(\n",
      "  in_features=64, out_features=32, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): _SpectralNorm()\n",
      "      (1): _BjorckNorm()\n",
      "    )\n",
      "  )\n",
      "), min=0.9999998211860657, max=1.0\n",
      "ParametrizedFrobeniusLinear(\n",
      "  in_features=32, out_features=1, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): _FrobeniusNorm()\n",
      "    )\n",
      "  )\n",
      "), min=0.9999999403953552, max=0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Before export ===\")\n",
    "layers = list(wass.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After export ===\n",
      "Linear(in_features=784, out_features=128, bias=True), min=0.9999998211860657, max=1.0\n",
      "Linear(in_features=128, out_features=64, bias=True), min=0.9999998211860657, max=1.0000001192092896\n",
      "Linear(in_features=64, out_features=32, bias=True), min=0.9999998211860657, max=1.0\n",
      "Linear(in_features=32, out_features=1, bias=True), min=0.9999999403953552, max=0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "wexport = wass.vanilla_export()\n",
    "\n",
    "print(\"=== After export ===\")\n",
    "layers = list(wexport.children())\n",
    "for layer in layers:\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        w = layer.weight\n",
    "        u, s, v = torch.svd(w)\n",
    "        print(f\"{layer}, min={s.min()}, max={s.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all our singular values are very close to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
